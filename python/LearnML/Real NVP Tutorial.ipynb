{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "endangered-complex",
   "metadata": {},
   "source": [
    "# Density Estimation Using Real NVP Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-warren",
   "metadata": {},
   "source": [
    "The basic idea behind Real NVP is to constructs a transformation/flow between latent variables and observed variables such that the transformation satisfies the following two conditions:\n",
    "\n",
    "1. It is invertible and its Jacobian matrix's determinant is easy to compute.\n",
    "2. It is flexible such that it can transform random variables of simple distributions into random variables of complex distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-paintball",
   "metadata": {},
   "source": [
    "First, we import the required modules for our functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "from sys import exit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as functional\n",
    "\n",
    "torch.set_default_dtype(torch.float64) #use double precision numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-scene",
   "metadata": {},
   "source": [
    "The transformation used in the real NVP method is a composition of multiple affine coupling transformations. In each affine coupling transformation, a subset of the random varaibles is kept the same and an affine transformation, parameterized using the fixed random varaibles, is applied to the remaining subset of the random variables. It is staightforward to show that the affine coupling transformation satifies the above two conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpha-harvard",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Affine_Coupling(nn.Module):\n",
    "    def __init__(self, mask, hidden_dim):\n",
    "        super(Affine_Coupling, self).__init__()\n",
    "        self.input_dim = len(mask)\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        ## mask to seperate positions that do not change and positions that change.\n",
    "        ## mask[i] = 1 means the ith position does not change.\n",
    "        self.mask = nn.Parameter(mask, requires_grad = False)\n",
    "\n",
    "        ## layers used to compute scale in affine transformation\n",
    "        self.scale_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.scale_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.scale_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "        self.scale = nn.Parameter(torch.Tensor(self.input_dim))\n",
    "        init.normal_(self.scale)\n",
    "\n",
    "        ## layers used to compute translation in affine transformation \n",
    "        self.translation_fc1 = nn.Linear(self.input_dim, self.hidden_dim)\n",
    "        self.translation_fc2 = nn.Linear(self.hidden_dim, self.hidden_dim)\n",
    "        self.translation_fc3 = nn.Linear(self.hidden_dim, self.input_dim)\n",
    "\n",
    "    def _compute_scale(self, x):\n",
    "        ## compute scaling factor using unchanged part of x with a neural network\n",
    "        s = torch.relu(self.scale_fc1(x*self.mask))\n",
    "        s = torch.relu(self.scale_fc2(s))\n",
    "        s = torch.relu(self.scale_fc3(s)) * self.scale        \n",
    "        return s\n",
    "\n",
    "    def _compute_translation(self, x):\n",
    "        ## compute translation using unchanged part of x with a neural network        \n",
    "        t = torch.relu(self.translation_fc1(x*self.mask))\n",
    "        t = torch.relu(self.translation_fc2(t))\n",
    "        t = self.translation_fc3(t)        \n",
    "        return t\n",
    "    \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variable to observed variable\n",
    "        s = self._compute_scale(x)\n",
    "        t = self._compute_translation(x)\n",
    "        \n",
    "        y = self.mask*x + (1-self.mask)*(x*torch.exp(s) + t)        \n",
    "        logdet = torch.sum((1 - self.mask)*s, -1)\n",
    "        \n",
    "        return y, logdet\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed varible to latent space variable\n",
    "        s = self._compute_scale(y)\n",
    "        t = self._compute_translation(y)\n",
    "                \n",
    "        x = self.mask*y + (1-self.mask)*((y - t)*torch.exp(-s))\n",
    "        logdet = torch.sum((1 - self.mask)*(-s), -1)\n",
    "        \n",
    "        return x, logdet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-gregory",
   "metadata": {},
   "source": [
    "The real NVP method can be applied to train models for estimating density in two different scenarios. In the first scenario, samples/data from the underlying distribution are available and we want to train a model to estimate the underlying distribution density. In the second scenario, we know the unnormalized distribution density function and do not know its normalization factor. In this case, we want to train a model such that samples from the trained model approximate the unnormalized distribution density. In the following, the real NVP method is applied to train models in both scenarios. To make things simple, the distributions shown here are only two dimensional distributions, but similar ideas can be applied to high dimensional distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP_2D(nn.Module):\n",
    "    '''\n",
    "    A vanilla RealNVP class for modeling 2 dimensional distributions\n",
    "    '''\n",
    "    def __init__(self, masks, hidden_dim):\n",
    "        '''\n",
    "        initialized with a list of masks. each mask define an affine coupling layer\n",
    "        '''\n",
    "        super(RealNVP_2D, self).__init__()        \n",
    "        self.hidden_dim = hidden_dim        \n",
    "        self.masks = nn.ParameterList(\n",
    "            [nn.Parameter(torch.Tensor(m),requires_grad = False)\n",
    "             for m in masks])\n",
    "\n",
    "        self.affine_couplings = nn.ModuleList(\n",
    "            [Affine_Coupling(self.masks[i], self.hidden_dim)\n",
    "             for i in range(len(self.masks))])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ## convert latent space variables into observed variables\n",
    "        y = x\n",
    "        logdet_tot = 0\n",
    "        for i in range(len(self.affine_couplings)):\n",
    "            y, logdet = self.affine_couplings[i](y)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## a normalization layer is added such that the observed variables is within\n",
    "        ## the range of [-4, 4].\n",
    "        logdet = torch.sum(torch.log(torch.abs(4*(1-(torch.tanh(y))**2))), -1)        \n",
    "        y = 4*torch.tanh(y)\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "        \n",
    "        return y, logdet_tot\n",
    "\n",
    "    def inverse(self, y):\n",
    "        ## convert observed variables into latent space variables        \n",
    "        x = y        \n",
    "        logdet_tot = 0\n",
    "\n",
    "        # inverse the normalization layer\n",
    "        logdet = torch.sum(torch.log(torch.abs(1.0/4.0* 1/(1-(x/4)**2))), -1)\n",
    "        x  = 0.5*torch.log((1+x/4)/(1-x/4))\n",
    "        logdet_tot = logdet_tot + logdet\n",
    "\n",
    "        ## inverse affine coupling layers\n",
    "        for i in range(len(self.affine_couplings)-1, -1, -1):\n",
    "            x, logdet = self.affine_couplings[i].inverse(x)\n",
    "            logdet_tot = logdet_tot + logdet\n",
    "            \n",
    "        return x, logdet_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-effort",
   "metadata": {},
   "source": [
    "## 1. Train models with the real NVP method using samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southwest-subject",
   "metadata": {},
   "source": [
    "Here, we use our real NVP method to train a model based on samples from the moon dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "returning-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Masks used to define the number and the type of affine coupling layers\n",
    "## In each mask, 1 means that the variable at the correspoding position is\n",
    "## kept fixed in the affine couling layer\n",
    "masks = [[1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0]]\n",
    "\n",
    "## dimenstion of hidden units used in scale and translation transformation\n",
    "hidden_dim = 128\n",
    "\n",
    "## construct the RealNVP_2D object\n",
    "realNVP = RealNVP_2D(masks, hidden_dim)\n",
    "if torch.cuda.device_count():\n",
    "    realNVP = realNVP.cuda()\n",
    "device = next(realNVP.parameters()).device\n",
    "\n",
    "optimizer = optim.Adam(realNVP.parameters(), lr = 0.0001)\n",
    "\n",
    "num_steps = 5000\n",
    "# num_steps = 5000\n",
    "\n",
    "## the following loop learns the RealNVP_2D model by data\n",
    "## in each loop, data is dynamically sampled from the scipy moon dataset\n",
    "for idx_step in range(num_steps):\n",
    "    ## sample data from the scipy moon dataset\n",
    "    X, label = datasets.make_moons(n_samples = 512, noise = 0.05)\n",
    "    X = torch.Tensor(X).to(device = device)\n",
    "\n",
    "    ## transform data X to latent space Z\n",
    "    z, logdet = realNVP.inverse(X)\n",
    "\n",
    "    ## calculate the negative loglikelihood of X\n",
    "    loss = torch.log(z.new_tensor([2*math.pi])) + torch.mean(torch.sum(0.5*z**2, -1) - logdet)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "\n",
    "    if (idx_step + 1) % 1000 == 0:\n",
    "        print(f\"idx_steps: {idx_step:}, loss: {loss.item():.5f}\")\n",
    "        \n",
    "## after learning, we can test if the model can transform\n",
    "## the moon data distribution into the normal distribution\n",
    "X, label = datasets.make_moons(n_samples = 1000, noise = 0.05)\n",
    "X = torch.Tensor(X).to(device = device)\n",
    "z, logdet_jacobian = realNVP.inverse(X)\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "X = X.cpu().detach().numpy()\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(X[label==0,0], X[label==0,1], \".\")\n",
    "plt.plot(X[label==1,0], X[label==1,1], \".\")\n",
    "plt.title(\"X sampled from Moon dataset\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(z[label==0,0], z[label==0,1], \".\")\n",
    "plt.plot(z[label==1,0], z[label==1,1], \".\")\n",
    "plt.title(\"Z transformed from X\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-preview",
   "metadata": {},
   "source": [
    "Samples from the moon dataset, shown in the left of the following figure, are used to train a model with the real NVP method. After the training, these samples can be transformed into samples of standard normal distribution shown on the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lyric-virus",
   "metadata": {},
   "outputs": [],
   "source": [
    "## after learning, we can also test if the model can transform\n",
    "## the normal distribution into the moon data distribution \n",
    "z = torch.normal(0, 1, size = (1000, 2)).to(device = device)\n",
    "X, _ = realNVP(z)\n",
    "X = X.cpu().detach().numpy()\n",
    "z = z.cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(2, figsize = (12.8, 4.8))\n",
    "fig.clf()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(z[:,0], z[:,1], \".\")\n",
    "plt.title(\"Z sampled from normal distribution\")\n",
    "plt.xlabel(r\"$z_1$\")\n",
    "plt.ylabel(r\"$z_2$\")\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(X[:,0], X[:,1], \".\")\n",
    "plt.title(\"X transformed from Z\")\n",
    "plt.xlabel(r\"$x_1$\")\n",
    "plt.ylabel(r\"$x_2$\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-nicaragua",
   "metadata": {},
   "source": [
    "The corresponding inverse transformation can transform samples of standard normal distributions into samples of the moon dataset distribution as shown in the following figure. The left figure shows samples from a standard normal distribution and the right figure shows the transformed samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arbitrary-wonder",
   "metadata": {},
   "source": [
    "## 2. Train models with the real NVP method using potential energy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-overhead",
   "metadata": {},
   "source": [
    "Here is just one of the models trained to approximate potential energies. In the training, only the potential energy functions are used. This potential energy functions are from the normalizing flow paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comprehensive-progressive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass an index to specify which potential to use\n",
    "# Here we just set potential_index = 1 to study the first graph\n",
    "\n",
    "#parser = argparse.ArgumentParser()\n",
    "#parser.add_argument(\"--potential_index\", type = int, choices = range(1,5), required = True)\n",
    "#args = parser.parse_args()\n",
    "\n",
    "potential_index = 1\n",
    "U = [compute_U1, compute_U2, compute_U3, compute_U4]\n",
    "compute_U = U[potential_index - 1]\n",
    "\n",
    "## Masks used to define the number and the type of affine coupling layers\n",
    "## In each mask, 1 means that the variable at the correspoding position is\n",
    "## kept fixed in the affine couling layer\n",
    "masks = [[1.0, 0.0],\n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],         \n",
    "         [0.0, 1.0],\n",
    "         [1.0, 0.0],\n",
    "         [0.0, 1.0]]\n",
    "\n",
    "## dimenstion of hidden units used in scale and translation transformation\n",
    "hidden_dim = 128\n",
    "\n",
    "## construct the RealNVP_2D object\n",
    "realNVP = RealNVP_2D(masks, hidden_dim)\n",
    "if torch.cuda.device_count():\n",
    "    realNVP = realNVP.cuda()\n",
    "    \n",
    "optimizer = optim.Adam(realNVP.parameters(), lr = 0.0001)\n",
    "num_steps = 5000 #you can adjust num_steps as nessesary\n",
    "\n",
    "## the following loop learns the RealNVP_2D model by potential energy\n",
    "## defined in the ./script/functions.py\n",
    "for idx_step in range(num_steps):\n",
    "    Z = torch.normal(0, 1, size = (1024, 2))\n",
    "    Z = Z.cpu() #change this to cuda if available\n",
    "    X, logdet = realNVP(Z)\n",
    "\n",
    "    logp = -compute_U(X)\n",
    "    loss = torch.mean(-logdet - logp)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (idx_step + 1) % 1000 == 0:\n",
    "        print(f\"idx_steps: {idx_step:}, loss: {loss.item():.5f}\")\n",
    "\n",
    "z = torch.normal(0, 1, size = (1000, 2))\n",
    "z = z.cpu()\n",
    "x, _ = realNVP(z)\n",
    "x = x.cpu().detach().numpy()\n",
    "z = z.cpu().detach().numpy()\n",
    "   \n",
    "fig = plt.figure(0)\n",
    "fig.clf()\n",
    "plt.plot(x[:,0], x[:,1], \".\")\n",
    "plt.xlabel(r'$X_1$')\n",
    "plt.ylabel(r'$X_2$')\n",
    "plt.xlim([-4,4])\n",
    "plt.ylim([-4,4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-divide",
   "metadata": {},
   "source": [
    "After training models, we can generate samples from the model by transforming samples of standard normal distribution. These samples are shown in the above figure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kandEnv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
